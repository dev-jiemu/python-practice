#!/usr/bin/env python3
"""
Python VAD Test Script
Go ì½”ë“œì™€ ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ VAD í•„í„°ë§ ìˆ˜í–‰
mp3 -> wav -> vad filter -> webm
"""

import sys
import os
import time
import subprocess
from pathlib import Path
import torch
import torchaudio
from silero_vad import load_silero_vad, get_speech_timestamps


def extract_audio_to_wav(input_path):
    """MP3/MP4ë¥¼ WAVë¡œ ë³€í™˜ (16kHz, mono)"""
    print(f"ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘: {input_path}")
    
    output_path = Path(input_path).with_suffix('.wav')
    
    cmd = [
        'ffmpeg',
        '-i', input_path,
        '-vn',
        '-c:a', 'pcm_s16le',  # 16-bit PCM
        '-ar', '16000',        # 16kHz
        '-ac', '1',            # mono
        '-map_metadata', '-1',
        '-f', 'wav',
        '-y',
        str(output_path)
    ]
    
    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode != 0:
        raise Exception(f"FFmpeg ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}")
    
    file_size_mb = os.path.getsize(output_path) / (1024 * 1024)
    print(f"ì¶”ì¶œëœ ì˜¤ë””ì˜¤ íŒŒì¼ í¬ê¸°: {file_size_mb:.2f} MB")
    print(f"ê²°ê³¼ íŒŒì¼: {output_path}")
    
    return str(output_path)


def vad_filter(wav_path):
    """VAD í•„í„°ë§ ì ìš©"""
    print(f"\n{'='*50}")
    print("VAD í•„í„°ë§ ì‹œì‘")
    print(f"{'='*50}")
    
    # Silero VAD ëª¨ë¸ ë¡œë“œ
    print("Silero VAD ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = load_silero_vad()
    
    # ì˜¤ë””ì˜¤ ë¡œë“œ
    print(f"ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ ì¤‘: {wav_path}")
    wav, sample_rate = torchaudio.load(wav_path)
    
    # monoë¡œ ë³€í™˜ (í˜¹ì‹œ stereoì¼ ê²½ìš°)
    if wav.shape[0] > 1:
        wav = wav.mean(dim=0, keepdim=True)
    
    wav = wav.squeeze()
    
    print(f"ìƒ˜í”Œë ˆì´íŠ¸: {sample_rate} Hz")
    print(f"ì±„ë„ ìˆ˜: 1 (mono)")
    print(f"ì´ ê¸¸ì´: {len(wav) / sample_rate:.2f}ì´ˆ")
    
    # ë°ì´í„° ë²”ìœ„ í™•ì¸
    print(f"ğŸ” PCM ë°ì´í„° ë²”ìœ„: min={wav.min():.6f}, max={wav.max():.6f}")
    print(f"ğŸ” ì²˜ìŒ 10ê°œ ìƒ˜í”Œ: {wav[:10].tolist()}")
    print(f"ğŸ” í‰ê·  ì ˆëŒ€ê°’: {wav.abs().mean():.6f}")
    
    # ìŒì„± êµ¬ê°„ íƒì§€
    print("\nìŒì„± êµ¬ê°„ì„ íƒì§€í•˜ëŠ” ì¤‘...")
    
    # Go ì½”ë“œì™€ ë™ì¼í•œ íŒŒë¼ë¯¸í„° ì‚¬ìš©
    speech_timestamps = get_speech_timestamps(
        wav,
        model,
        threshold=0.5,                    # Go: Threshold
        min_silence_duration_ms=100,      # Go: MinSilenceDurationMs
        speech_pad_ms=30,                 # Go: SpeechPadMs
        min_speech_duration_ms=0,         # ëª…ì‹œ: ìµœì†Œ ìŒì„± ê¸¸ì´ ì œí•œ ì—†ìŒ
        max_speech_duration_s=float('inf'),  # ëª…ì‹œ: ìµœëŒ€ ìŒì„± ê¸¸ì´ ì œí•œ ì—†ìŒ
        return_seconds=True
    )
    
    print(f"íƒì§€ëœ ìŒì„± êµ¬ê°„: {len(speech_timestamps)}ê°œ")
    print(f"ğŸ” ì¶”ê°€ í•„í„°ë§ íŒŒë¼ë¯¸í„°: min_speech_duration_ms=0, max_speech_duration_s=inf")
    
    # ë³‘í•© ë¡œì§ ì œê±° - Python VADì˜ ì›ë˜ ê²°ê³¼ë§Œ ì‚¬ìš©
    merged_segments = speech_timestamps
    
    print(f"ìŒì„± êµ¬ê°„ (ë³‘í•© ì—†ìŒ): {len(merged_segments)}ê°œ\n")
    
    # ìŒì„± êµ¬ê°„ ì¶œë ¥
    for i, segment in enumerate(merged_segments):
        duration = segment['end'] - segment['start']
        print(f"ìŒì„± êµ¬ê°„ {i+1}: {segment['start']:.2f}s ~ {segment['end']:.2f}s ({duration:.2f}s)")
    
    # ì˜¤ë””ì˜¤ í•„í„°ë§ (ê¸´ ë¬´ìŒë§Œ ì œê±°)
    print("\nì˜¤ë””ì˜¤ í•„í„°ë§ ì¤‘...")
    processed_audio = wav.clone()
    
    if len(merged_segments) == 0:
        print("âš ï¸  ìŒì„± êµ¬ê°„ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì „ì²´ë¥¼ ë¬´ìŒ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        processed_audio[:] = 0.0
    else:
        # ìŒì„± êµ¬ê°„ ë§ˆí‚¹
        speech_mask = torch.zeros(len(wav), dtype=torch.bool)
        
        for segment in merged_segments:
            start_sample = int(segment['start'] * sample_rate)
            end_sample = int(segment['end'] * sample_rate)
            
            # ë²”ìœ„ ì²´í¬
            start_sample = max(0, start_sample)
            end_sample = min(len(wav), end_sample)
            
            if start_sample < end_sample:
                speech_mask[start_sample:end_sample] = True
        
        # ê¸´ ë¬´ìŒ êµ¬ê°„ë§Œ ì œê±° (1ì´ˆ ì´ìƒ)
        long_silence_threshold = 1.0
        silenced_samples = 0
        
        in_silence = False
        silence_start = 0
        
        for i in range(len(processed_audio)):
            if not speech_mask[i]:
                if not in_silence:
                    in_silence = True
                    silence_start = i
                
                # ë§ˆì§€ë§‰ì´ê±°ë‚˜ ë‹¤ìŒì´ ìŒì„±ì´ë©´
                if i == len(processed_audio) - 1 or speech_mask[i + 1]:
                    silence_length = (i - silence_start + 1) / sample_rate
                    
                    # ê¸´ ë¬´ìŒë§Œ 0ìœ¼ë¡œ ì²˜ë¦¬
                    if silence_length >= long_silence_threshold:
                        processed_audio[silence_start:i+1] = 0.0
                        silenced_samples += (i - silence_start + 1)
                        print(f"ğŸ”‡ ê¸´ ë¬´ìŒ ì œê±°: {silence_start/sample_rate:.2f}s ~ {i/sample_rate:.2f}s ({silence_length:.2f}s)")
                    
                    in_silence = False
        
        original_duration = len(wav) / sample_rate
        speech_duration = speech_mask.sum().item() / sample_rate
        silenced_duration = silenced_samples / sample_rate
        
        print(f"\nğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
        print(f"ì „ì²´ ê¸¸ì´: {original_duration:.2f}ì´ˆ (ìœ ì§€ë¨)")
        print(f"ìŒì„± êµ¬ê°„: {speech_duration:.2f}ì´ˆ ({speech_duration/original_duration*100:.1f}%)")
        print(f"ê¸´ ë¬´ìŒ ì œê±°: {silenced_duration:.2f}ì´ˆ ({silenced_duration/original_duration*100:.1f}%)")
        print(f"ì§§ì€ ë¬´ìŒ ìœ ì§€: {original_duration - speech_duration - silenced_duration:.2f}ì´ˆ")
    
    # í•„í„°ë§ëœ WAV ì €ì¥
    output_wav_path = str(Path(wav_path).with_stem(Path(wav_path).stem + '_vad_filtered'))
    
    # Float32ë¥¼ Int16ìœ¼ë¡œ ë³€í™˜
    processed_audio_int16 = (processed_audio * 32767).clamp(-32768, 32767).to(torch.int16)
    
    torchaudio.save(
        output_wav_path,
        processed_audio_int16.unsqueeze(0),
        sample_rate,
        encoding='PCM_S',
        bits_per_sample=16
    )
    
    print(f"\nâœ… ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ì¶œë ¥ íŒŒì¼: {output_wav_path}")
    print(f"ğŸ“ 1ì´ˆ ì´ìƒ ê¸´ ë¬´ìŒë§Œ ì œê±°, ì§§ì€ ë¬´ìŒì€ ì›ë³¸ ìœ ì§€ (íƒ€ì„ìŠ¤íƒ¬í”„ ë³´ì¡´)")
    
    return merged_segments, output_wav_path


def extract_audio_to_webm(filtered_wav_path):
    """í•„í„°ë§ëœ WAVë¥¼ WebMìœ¼ë¡œ ë³€í™˜"""
    print(f"\nì˜¤ë””ì˜¤ ë³€í™˜ ì¤‘: WAV -> WebM")
    
    output_path = str(Path(filtered_wav_path).with_stem(
        Path(filtered_wav_path).stem + '_extracted'
    ).with_suffix('.webm'))
    
    cmd = [
        'ffmpeg',
        '-i', filtered_wav_path,
        '-c:a', 'libopus',
        '-b:a', '12k',
        '-application', 'voip',  # ìŒì„± ìµœì í™”
        '-f', 'webm',
        '-y',
        output_path
    ]
    
    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode != 0:
        raise Exception(f"FFmpeg ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}")
    
    file_size_mb = os.path.getsize(output_path) / (1024 * 1024)
    print(f"ì¶”ì¶œëœ ì˜¤ë””ì˜¤ íŒŒì¼ í¬ê¸°: {file_size_mb:.2f} MB")
    print(f"ê²°ê³¼ íŒŒì¼: {output_path}")
    
    return output_path


def main():
    if len(sys.argv) != 2:
        print(f"ì‚¬ìš©ë²•: {sys.argv[0]} <ì…ë ¥íŒŒì¼.mp3|mp4>")
        sys.exit(1)
    
    input_file = sys.argv[1]
    
    if not os.path.exists(input_file):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        sys.exit(1)
    
    print(f"{'='*50}")
    print("Python VAD í•„í„° í…ŒìŠ¤íŠ¸")
    print(f"{'='*50}")
    print(f"ì…ë ¥ íŒŒì¼: {input_file}\n")
    
    total_start = time.time()
    
    # 1. MP3/MP4 -> WAV
    wav_extract_start = time.time()
    wav_path = extract_audio_to_wav(input_file)
    wav_extract_duration = time.time() - wav_extract_start
    print(f"â±ï¸  ì˜¤ë””ì˜¤ ì¶”ì¶œ ì‹œê°„(mp4 to wav): {wav_extract_duration:.2f}ì´ˆ\n")
    
    # 2. VAD í•„í„°ë§
    filter_start = time.time()
    segments, filtered_wav_path = vad_filter(wav_path)
    filter_duration = time.time() - filter_start
    print(f"\nâ±ï¸  ë¬´ìŒêµ¬ê°„ ë³€í™˜ ì‹œê°„: {filter_duration:.2f}ì´ˆ")
    
    # 3. WAV -> WebM
    extract_start = time.time()
    webm_path = extract_audio_to_webm(filtered_wav_path)
    extract_duration = time.time() - extract_start
    print(f"\nâ±ï¸  ì˜¤ë””ì˜¤ ì¶”ì¶œ ì‹œê°„(wav to webm): {extract_duration:.2f}ì´ˆ")
    
    # ì „ì²´ ì‹¤í–‰ ì‹œê°„
    total_duration = time.time() - total_start
    
    print(f"\n{'='*50}")
    print("ğŸ“Š Results")
    print(f"{'='*50}")
    print(f"ì˜¤ë””ì˜¤ ì¶”ì¶œ(mp4 to wav):    {wav_extract_duration:8.2f}ì´ˆ ({wav_extract_duration/total_duration*100:5.1f}%)")
    print(f"VAD í•„í„°ë§:                {filter_duration:8.2f}ì´ˆ ({filter_duration/total_duration*100:5.1f}%)")
    print(f"ì˜¤ë””ì˜¤ ë³€í™˜(wav to webm):   {extract_duration:8.2f}ì´ˆ ({extract_duration/total_duration*100:5.1f}%)")
    print(f"{'='*50}")
    print(f"ì „ì²´ ì‹¤í–‰ ì‹œê°„:              {total_duration:8.2f}ì´ˆ (100.0%)")
    
    print(f"\nâœ… ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ğŸ“ ìµœì¢… ê²°ê³¼ íŒŒì¼: {webm_path}")


if __name__ == '__main__':
    main()
