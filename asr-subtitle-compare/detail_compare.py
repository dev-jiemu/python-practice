from difflib import SequenceMatcher
from typing import List, Dict, Set, Tuple
import json
from utils import parse_srt, normalize_text, SRTSegment

def find_all_overlapping_segments(seg: SRTSegment, segments: List[SRTSegment], threshold: float = 0.2) -> List[SRTSegment]:
    """ì‹œê°„ì ìœ¼ë¡œ ê²¹ì¹˜ëŠ” ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ ì°¾ê¸° (1:N ë§¤ì¹­ ê°€ëŠ¥)"""
    overlapping = []
    for other_seg in segments:
        overlap_start = max(seg.start_time, other_seg.start_time)
        overlap_end = min(seg.end_time, other_seg.end_time)
        overlap_duration = max(0, overlap_end - overlap_start)

        seg_duration = seg.end_time - seg.start_time
        other_duration = other_seg.end_time - other_seg.start_time

        if seg_duration > 0 and other_duration > 0:
            # ë‘˜ ì¤‘ ì§§ì€ ì„¸ê·¸ë¨¼íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ê²¹ì¹¨ ë¹„ìœ¨ ê³„ì‚°
            overlap_ratio = overlap_duration / min(seg_duration, other_duration)
            if overlap_ratio >= threshold:
                overlapping.append(other_seg)

    return overlapping

def find_text_based_match(seg: SRTSegment, segments: List[SRTSegment], threshold: float = 0.85) -> Tuple[SRTSegment, float]:
    text1_norm = normalize_text(seg.text)

    best_match = None
    best_similarity = 0.0

    for other_seg in segments:
        text2_norm = normalize_text(other_seg.text)
        similarity = SequenceMatcher(None, text1_norm, text2_norm).ratio()

        if similarity > best_similarity and similarity >= threshold:
            best_similarity = similarity
            best_match = other_seg

    return best_match, best_similarity

def detect_segment_patterns(segments1: List[SRTSegment], segments2: List[SRTSegment]):
    """ì„¸ê·¸ë¨¼íŠ¸ ë³‘í•©/ë¶„í•  íŒ¨í„´ ê°ì§€"""

    patterns = {
        'one_to_one': [],      # 1:1 ë§¤ì¹­
        'merged': [],           # N:1 (ì—¬ëŸ¬ê°œê°€ í•˜ë‚˜ë¡œ í•©ì³ì§)
        'split': [],            # 1:N (í•˜ë‚˜ê°€ ì—¬ëŸ¬ê°œë¡œ ìª¼ê°œì§)
        'missing': [],          # ë§¤ì¹­ ì—†ìŒ (ì‹¤ì œ ì†ì‹¤)
        'text_diff_only': [],   # ì„¸ê·¸ë¨¼íŠ¸ëŠ” ë§¤ì¹­ë˜ì§€ë§Œ í…ìŠ¤íŠ¸ë§Œ ë‹¤ë¦„
        'timeline_mismatch': [], # íƒ€ì„ë¼ì¸ì´ ë‹¤ë¦„
    }

    matched_seg2_indices = set()

    for seg1 in segments1:
        overlapping = find_all_overlapping_segments(seg1, segments2, threshold=0.2)

        if len(overlapping) == 0:
            # ë§¤ì¹­ë˜ëŠ” ì„¸ê·¸ë¨¼íŠ¸ê°€ ì—†ìŒ
            text_match, similarity = find_text_based_match(seg1, segments2, threshold=0.85)
            if text_match:
                # íƒ€ì„ë¼ì¸ì€ ì•ˆ ë§ì§€ë§Œ í…ìŠ¤íŠ¸ëŠ” ì¼ì¹˜
                matched_seg2_indices.add(text_match.index)
                patterns['timeline_mismatch'].append({
                    'seg1': seg1,
                    'seg2': text_match,
                    'similarity': similarity,
                    'time_diff_ms': abs((seg1.start_time + seg1.end_time)/2 -
                                        (text_match.start_time + text_match.end_time)/2)
                })
            else:
                patterns['missing'].append({
                    'seg1': seg1,
                    'seg2_list': []
                })

        elif len(overlapping) == 1:
            # 1:1 ë§¤ì¹­
            seg2 = overlapping[0]
            matched_seg2_indices.add(seg2.index)

            text1_norm = normalize_text(seg1.text)
            text2_norm = normalize_text(seg2.text)
            similarity = SequenceMatcher(None, text1_norm, text2_norm).ratio()

            if similarity < 0.95:  # 5% ì´ìƒ ì°¨ì´
                patterns['text_diff_only'].append({
                    'seg1': seg1,
                    'seg2': seg2,
                    'similarity': similarity
                })
            else:
                patterns['one_to_one'].append({
                    'seg1': seg1,
                    'seg2': seg2,
                    'similarity': similarity
                })
        else:
            # 1:N ë§¤ì¹­ ì „ì— í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ì²´í¬
            exact_match = None
            for seg2 in overlapping:
                text1_norm = normalize_text(seg1.text)
                text2_norm = normalize_text(seg2.text)
                if SequenceMatcher(None, text1_norm, text2_norm).ratio() > 0.95:
                    exact_match = seg2
                    break

            if exact_match:
                # ì‹¤ì œë¡œëŠ” 1:1 ë§¤ì¹­
                matched_seg2_indices.add(exact_match.index)
                patterns['one_to_one'].append({
                    'seg1': seg1,
                    'seg2': exact_match,
                    'similarity': 1.0
                })
            else:
                # ì§„ì§œ ë¶„í• 
                for seg2 in overlapping:
                    matched_seg2_indices.add(seg2.index)
                patterns['split'].append({
                    'seg1': seg1,
                    'seg2_list': overlapping
                })

    # segments2ì—ì„œ ë§¤ì¹­ ì•ˆ ëœ ê²ƒë“¤ ì°¾ê¸° (ì—­ë°©í–¥ ì²´í¬ - ë³‘í•© ê°ì§€)
    for seg2 in segments2:
        if seg2.index not in matched_seg2_indices:
            # ì´ seg2ì™€ ê²¹ì¹˜ëŠ” seg1ë“¤ ì°¾ê¸°
            overlapping_seg1 = find_all_overlapping_segments(seg2, segments1, threshold=0.2)

            if len(overlapping_seg1) > 1:
                # N:1 ë§¤ì¹­ (ë³‘í•©)
                patterns['merged'].append({
                    'seg1_list': overlapping_seg1,
                    'seg2': seg2
                })

    return patterns

def analyze_segment_patterns(patterns: Dict):
    """íŒ¨í„´ ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„±"""

    print("="*80)
    print("  ğŸ” ì„¸ê·¸ë¨¼íŠ¸ ë§¤ì¹­ íŒ¨í„´ ë¶„ì„")
    print("="*80)
    print()

    print("ã€ ë§¤ì¹­ íŒ¨í„´ í†µê³„ ã€‘")
    print(f"  âœ… 1:1 ë§¤ì¹­ (ê±°ì˜ ë™ì¼): {len(patterns['one_to_one'])}ê°œ")
    print(f"  ğŸ“ 1:1 ë§¤ì¹­ (í…ìŠ¤íŠ¸ ì°¨ì´): {len(patterns['text_diff_only'])}ê°œ")
    print(f"  ğŸ”€ ë¶„í•  (1â†’N): {len(patterns['split'])}ê°œ")
    print(f"  ğŸ”— ë³‘í•© (Nâ†’1): {len(patterns['merged'])}ê°œ")
    print(f"  âŒ ë§¤ì¹­ ì—†ìŒ (ì‹¤ì œ ì†ì‹¤): {len(patterns['missing'])}ê°œ")
    print(f"  â±ï¸ íƒ€ì„ë¼ì¸ ë¶ˆì¼ì¹˜ (í…ìŠ¤íŠ¸ ì¼ì¹˜): {len(patterns['timeline_mismatch'])}ê°œ")
    print()

    # ë¶„í•  ìƒì„¸ ë¶„ì„
    if patterns['split']:
        print("ã€ ë¶„í•  ì‚¬ë¡€ ìƒì„¸ (ìƒìœ„ 5ê°œ) ã€‘")
        for i, item in enumerate(patterns['split'][:5], 1):
            seg1 = item['seg1']
            seg2_list = item['seg2_list']
            print(f"\n{i}. ì›ë³¸ 1ê°œ â†’ VAD {len(seg2_list)}ê°œë¡œ ë¶„í• ")
            print(f"   ì›ë³¸ #{seg1.index}: [{seg1.time_str()}]")
            print(f"   â†’ {seg1.text}")
            print(f"   VAD ì„¸ê·¸ë¨¼íŠ¸ë“¤:")
            for seg2 in seg2_list:
                print(f"     #{seg2.index}: [{seg2.time_str()}]")
                print(f"     â†’ {seg2.text}")
        print()

    # ë³‘í•© ìƒì„¸ ë¶„ì„
    if patterns['merged']:
        print("ã€ ë³‘í•© ì‚¬ë¡€ ìƒì„¸ (ìƒìœ„ 5ê°œ) ã€‘")
        for i, item in enumerate(patterns['merged'][:5], 1):
            seg1_list = item['seg1_list']
            seg2 = item['seg2']
            print(f"\n{i}. ì›ë³¸ {len(seg1_list)}ê°œ â†’ VAD 1ê°œë¡œ ë³‘í•©")
            print(f"   ì›ë³¸ ì„¸ê·¸ë¨¼íŠ¸ë“¤:")
            for seg1 in seg1_list:
                print(f"     #{seg1.index}: [{seg1.time_str()}]")
                print(f"     â†’ {seg1.text}")
            print(f"   VAD #{seg2.index}: [{seg2.time_str()}]")
            print(f"   â†’ {seg2.text}")
        print()

    # ì‹¤ì œ ì†ì‹¤ ë¶„ì„
    if patterns['missing']:
        print("ã€ ì‹¤ì œ ì†ì‹¤ ì‚¬ë¡€ (ìƒìœ„ 5ê°œ) ã€‘")
        for i, item in enumerate(patterns['missing'][:5], 1):
            seg1 = item['seg1']
            print(f"\n{i}. ì›ë³¸ #{seg1.index}: [{seg1.time_str()}]")
            print(f"   â†’ {seg1.text}")
        print()

    # í…ìŠ¤íŠ¸ ì°¨ì´ë§Œ ìˆëŠ” ê²½ìš°
    if patterns['text_diff_only']:
        print("ã€ í…ìŠ¤íŠ¸ ì°¨ì´ ì‚¬ë¡€ (ìœ ì‚¬ë„ ë‚®ì€ ìƒìœ„ 5ê°œ) ã€‘")
        sorted_diffs = sorted(patterns['text_diff_only'], key=lambda x: x['similarity'])

        for i, item in enumerate(sorted_diffs[:5], 1):
            seg1 = item['seg1']
            seg2 = item['seg2']
            sim = item['similarity']
            print(f"\n{i}. ìœ ì‚¬ë„ {sim*100:.1f}% (#{seg1.index} â†” #{seg2.index})")
            print(f"   ì›ë³¸: {seg1.text}")
            print(f"   VAD:  {seg2.text}")
        print()

    # íƒ€ì„ë¼ì¸ ë¶ˆì¼ì¹˜ ìƒì„¸ì²´í¬
    if patterns['timeline_mismatch']:
        print("ã€ íƒ€ì„ë¼ì¸ ë¶ˆì¼ì¹˜ ì‚¬ë¡€ (ìƒìœ„ 5ê°œ) ã€‘")
        for i, item in enumerate(patterns['timeline_mismatch'][:5], 1):
            seg1 = item['seg1']
            seg2 = item['seg2']
            time_diff = item['time_diff_ms'] / 1000
            print(f"\n{i}. ì‹œê°„ ì°¨ì´ {time_diff:.2f}ì´ˆ (ìœ ì‚¬ë„ {item['similarity']*100:.1f}%)")
            print(f"   ì›ë³¸ #{seg1.index}: [{seg1.time_str()}]")
            print(f"   â†’ {seg1.text}")
            print(f"   VAD #{seg2.index}: [{seg2.time_str()}]")
            print(f"   â†’ {seg2.text}")
        print()

def generate_advanced_report(file1: str, file2: str, label1: str, label2: str, output_file: str = None):
    segments1 = parse_srt(file1)
    segments2 = parse_srt(file2)

    print("="*80)
    print(f"  ê³ ê¸‰ ë¹„êµ ë¦¬í¬íŠ¸: {label1} vs {label2}")
    print("="*80)
    print()

    # ì „ì²´ í†µê³„
    total_words1 = sum(len(normalize_text(s.text).split()) for s in segments1)
    total_words2 = sum(len(normalize_text(s.text).split()) for s in segments2)

    print(f"ğŸ“Š ê¸°ë³¸ í†µê³„:")
    print(f"   {label1}: {len(segments1)}ê°œ ì„¸ê·¸ë¨¼íŠ¸, {total_words1} ë‹¨ì–´")
    print(f"   {label2}: {len(segments2)}ê°œ ì„¸ê·¸ë¨¼íŠ¸, {total_words2} ë‹¨ì–´")
    print(f"   ì„¸ê·¸ë¨¼íŠ¸ ì°¨ì´: {len(segments1) - len(segments2):+d}ê°œ")
    print(f"   ë‹¨ì–´ ì°¨ì´: {total_words1 - total_words2:+d}ê°œ")
    print(f"   ë‹¨ì–´ ì†ì‹¤ë¥ : {((total_words1-total_words2)/total_words1*100):.1f}%")
    print()

    # íŒ¨í„´ ê°ì§€
    patterns = detect_segment_patterns(segments1, segments2)

    # íŒ¨í„´ ë¶„ì„
    analyze_segment_patterns(patterns)

    # JSON ì €ì¥
    if output_file:
        report_data = {
            'summary': {
                'file1': file1,
                'file2': file2,
                'label1': label1,
                'label2': label2,
                'segments1': len(segments1),
                'segments2': len(segments2),
                'words1': total_words1,
                'words2': total_words2,
                'word_loss_rate': ((total_words1-total_words2)/total_words1*100)
            },
            'patterns': {
                'one_to_one_count': len(patterns['one_to_one']),
                'text_diff_count': len(patterns['text_diff_only']),
                'split_count': len(patterns['split']),
                'merged_count': len(patterns['merged']),
                'missing_count': len(patterns['missing'])
            },
            'details': {
                'missing': [
                    {
                        'seg1': {
                            'index': item['seg1'].index,
                            'time': item['seg1'].time_str(),
                            'text': item['seg1'].text
                        }
                    }
                    for item in patterns['missing']
                ],
                'text_diff_only': [
                    {
                        'seg1': {
                            'index': item['seg1'].index,
                            'time': item['seg1'].time_str(),
                            'text': item['seg1'].text
                        },
                        'seg2': {
                            'index': item['seg2'].index,
                            'time': item['seg2'].time_str(),
                            'text': item['seg2'].text
                        },
                        'similarity': item['similarity']
                    }
                    for item in patterns['text_diff_only']
                ],
                'split': [
                    {
                        'seg1': {
                            'index': item['seg1'].index,
                            'time': item['seg1'].time_str(),
                            'text': item['seg1'].text
                        },
                        'seg2_list': [
                            {
                                'index': seg.index,
                                'time': seg.time_str(),
                                'text': seg.text
                            }
                            for seg in item['seg2_list']
                        ]
                    }
                    for item in patterns['split']
                ],
                'merged': [
                    {
                        'seg1_list': [
                            {
                                'index': seg.index,
                                'time': seg.time_str(),
                                'text': seg.text
                            }
                            for seg in item['seg1_list']
                        ],
                        'seg2': {
                            'index': item['seg2'].index,
                            'time': item['seg2'].time_str(),
                            'text': item['seg2'].text
                        }
                    }
                    for item in patterns['merged']
                ]
            }
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)

        print("="*80)
        print(f"âœ… ë¦¬í¬íŠ¸ê°€ '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("="*80)


if __name__ == '__main__':
    diff_srt_1 = "./compare/canary.srt"
    diff_srt_1_label = "Canary"
    diff_srt_2 = "./compare/canary_filtered.srt"
    diff_srt_2_label = "Canary VAD Filter"
    output_file = "./detailed_report_canary.json"

    generate_advanced_report(diff_srt_1, diff_srt_2, diff_srt_1_label, diff_srt_2_label, output_file)

